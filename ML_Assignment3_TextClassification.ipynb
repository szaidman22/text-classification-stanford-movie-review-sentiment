{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL9SI_EmHH-j"
   },
   "source": [
    "# Assignment 3: Text Classification Using the Stanford SST Sentiment Dataset\n",
    "## Sofia Zaidman\n",
    "## 4/15/23\n",
    "### https://github.com/szaidman22/text-classification-stanford-movie-review-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqI3aygmHbGz"
   },
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TsV5zV0sGyVb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install aimodelshare library\n",
    "! pip install aimodelshare==0.0.189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN4iOkUSHGak",
    "outputId": "20b123ec-6dad-48fa-bca8-1bd37f93b46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading [=============================================>   ]\n",
      "\n",
      "Data downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get competition data\n",
    "from aimodelshare import download_data\n",
    "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MJvAaruHjiJ",
    "outputId": "145aedbb-0c78-418d-99a3-81b3c4f12164"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Rock is destined to be the 21st Century 's...\n",
       "1    The gorgeously elaborate continuation of `` Th...\n",
       "2    Singer/composer Bryan Adams contributes a slew...\n",
       "3                 Yet the act is still charming here .\n",
       "4    Whether or not you 're enlightened by any of D...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up X_train, X_test, and y_train_labels objects\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
    "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
    "\n",
    "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
    "\n",
    "# ohe encode Y data\n",
    "y_train = pd.get_dummies(y_train_labels)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPxD1kLBIlOa",
    "outputId": "97c708d7-f26f-4bd3-a42d-6aa85017d18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "1821\n",
      "['Positive' 'Negative']\n",
      "A different movie -- sometimes tedious -- by a director many viewers would like to skip but film buffs should get to know .\n"
     ]
    }
   ],
   "source": [
    "#checking size and makeup of data\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(y_train_labels.unique())\n",
    "print(X_train[738])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPmnkPNUIV2A"
   },
   "source": [
    "## Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8I-VWHVIiZu"
   },
   "source": [
    "The dataset contains about 9000 total movie reviews that are labeled as having either positive or negative sentiment. \n",
    "\n",
    "Building a model to predict the sentiment of movie reviews would be practically useful for websites like Rotten Tomatoes or IMDB, which aggregate reviews from multiple sources and generate an overall rating for a film. It could also be useful as an input for a movie recommendation algorithm. \n",
    "\n",
    "Movie review websites would obviously benefit from a model like this, and so would the filmmakers, movie studios, and moviegoers who use aggregated review information to make choices about which films to make or see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pJWO-uPM6hh"
   },
   "source": [
    "## Run at least three prediction models to try to predict the SST sentiment dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3jhaN0sNaTf"
   },
   "source": [
    "First we need to preprocess and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FRd-zEwNY3F",
    "outputId": "e29a5a21-5294-4ae8-8ac3-de73a4ecdd87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 40)\n",
      "(1821, 40)\n"
     ]
    }
   ],
   "source": [
    "# This preprocessor function makes use of the tf.keras tokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Build vocabulary from training text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# preprocessor tokenizes words and makes sure all documents have the same length\n",
    "def preprocessor(data, maxlen=40, max_words=10000):\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    return X\n",
    "\n",
    "print(preprocessor(X_train).shape)\n",
    "print(preprocessor(X_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x8yVK8ENM3_"
   },
   "source": [
    "### Use an Embedding layer and LSTM layers in at least one model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqA0CtT6R153"
   },
   "source": [
    "I tried a couple simple versions with LSTM and I decided to keep the dropout in this one. It seems to prevent more severe initial overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KaALOEKTJAv"
   },
   "source": [
    "I submitted this model as #149 and it performed relatively well, with an accuracy of .807 on the test data. At the time of submission it was in the top 10 models, though to be fair many of the models submitted have almost identical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZccokvUNSLK",
    "outputId": "58c804a2-2e56-413b-d80b-9250697e0b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 40, 10)            100000    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                5504      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,570\n",
      "Trainable params: 105,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "173/173 [==============================] - 12s 56ms/step - loss: 0.6621 - acc: 0.6143 - val_loss: 0.9153 - val_acc: 0.1488\n",
      "Epoch 2/15\n",
      "173/173 [==============================] - 7s 41ms/step - loss: 0.5730 - acc: 0.6864 - val_loss: 0.6790 - val_acc: 0.6965\n",
      "Epoch 3/15\n",
      "173/173 [==============================] - 7s 40ms/step - loss: 0.4605 - acc: 0.7990 - val_loss: 0.6378 - val_acc: 0.7355\n",
      "Epoch 4/15\n",
      "173/173 [==============================] - 9s 54ms/step - loss: 0.3716 - acc: 0.8436 - val_loss: 0.6135 - val_acc: 0.7471\n",
      "Epoch 5/15\n",
      "173/173 [==============================] - 6s 36ms/step - loss: 0.3159 - acc: 0.8692 - val_loss: 0.6137 - val_acc: 0.7406\n",
      "Epoch 6/15\n",
      "173/173 [==============================] - 9s 53ms/step - loss: 0.2704 - acc: 0.8920 - val_loss: 0.6708 - val_acc: 0.7269\n",
      "Epoch 7/15\n",
      "173/173 [==============================] - 7s 42ms/step - loss: 0.2406 - acc: 0.9019 - val_loss: 0.6243 - val_acc: 0.7464\n",
      "Epoch 8/15\n",
      "173/173 [==============================] - 7s 41ms/step - loss: 0.2125 - acc: 0.9147 - val_loss: 0.5013 - val_acc: 0.7970\n",
      "Epoch 9/15\n",
      "173/173 [==============================] - 10s 55ms/step - loss: 0.1967 - acc: 0.9180 - val_loss: 0.5813 - val_acc: 0.7608\n",
      "Epoch 10/15\n",
      "173/173 [==============================] - 6s 36ms/step - loss: 0.1795 - acc: 0.9274 - val_loss: 0.5968 - val_acc: 0.7753\n",
      "Epoch 11/15\n",
      "173/173 [==============================] - 9s 53ms/step - loss: 0.1587 - acc: 0.9364 - val_loss: 0.6552 - val_acc: 0.7514\n",
      "Epoch 12/15\n",
      "173/173 [==============================] - 7s 42ms/step - loss: 0.1489 - acc: 0.9411 - val_loss: 0.7751 - val_acc: 0.7305\n",
      "Epoch 13/15\n",
      "173/173 [==============================] - 7s 41ms/step - loss: 0.1367 - acc: 0.9464 - val_loss: 0.6733 - val_acc: 0.7623\n",
      "Epoch 14/15\n",
      "173/173 [==============================] - 10s 55ms/step - loss: 0.1241 - acc: 0.9514 - val_loss: 0.9208 - val_acc: 0.7276\n",
      "Epoch 15/15\n",
      "173/173 [==============================] - 6s 36ms/step - loss: 0.1185 - acc: 0.9541 - val_loss: 0.6878 - val_acc: 0.7717\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 10, input_length=40))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=15,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeMfm55WSQYn",
    "outputId": "f3a8a8bc-4e43-4f71-b9e3-2a29260940d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kDLjA1piSRnq"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FSXFBAkSX9X",
    "outputId": "f79ee9c4-8756-4371-9a73-f2243a0d6c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Modelshare Username:··········\n",
      "AI Modelshare Password:··········\n",
      "AI Model Share login credentials set successfully.\n"
     ]
    }
   ],
   "source": [
    "#Set credentials using modelshare.org username/password\n",
    "\n",
    "from aimodelshare.aws import set_credentials\n",
    "    \n",
    "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
    "\n",
    "set_credentials(apiurl=apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "shXHINyTSiMI"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyE1TJn5SkzH",
    "outputId": "18bdbbea-83f2-40e0-86f6-240ed4297d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 9ms/step\n",
      "Insert search tags to help users find your model (optional): LSTM\n",
      "Provide any useful notes about your model (optional): LSTM with dropout\n",
      "\n",
      "Your model has been submitted as model version 149\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 1: \n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhuGUCdkTnS8"
   },
   "source": [
    "### Use an Embedding layer and Conv1d layers in at least one model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gjSHlysY3XW"
   },
   "source": [
    "Submitted this one and it performed worse than the first, but only slightly. This one learned much faster and got to 100% accuracy on training data very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc25ydAdTyq-",
    "outputId": "e3cee657-3559-465e-c8ec-eb7b65caec57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_29 (Embedding)    (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " conv1d_43 (Conv1D)          (None, 37, 16)            6416      \n",
      "                                                                 \n",
      " conv1d_44 (Conv1D)          (None, 34, 8)             520       \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 272)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 2)                 546       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,007,482\n",
      "Trainable params: 1,007,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "173/173 [==============================] - 3s 13ms/step - loss: 0.6637 - acc: 0.6158 - val_loss: 0.8886 - val_acc: 0.1488\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 3s 17ms/step - loss: 0.4957 - acc: 0.7504 - val_loss: 0.7069 - val_acc: 0.6575\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 3s 18ms/step - loss: 0.2892 - acc: 0.8808 - val_loss: 0.6459 - val_acc: 0.7016\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 3s 18ms/step - loss: 0.1852 - acc: 0.9290 - val_loss: 0.5270 - val_acc: 0.7753\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.1109 - acc: 0.9615 - val_loss: 0.4625 - val_acc: 0.8100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(10000, 100, input_length=40))\n",
    "model2.add(layers.Conv1D(16, 4, activation='relu'))  \n",
    "BatchNormalization(),\n",
    "model2.add(layers.Conv1D(8, 4, activation='relu'))  \n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model2.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kSJfRtj8Xjm1"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model2, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model2.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "at6tpikqXuxD",
    "outputId": "4cd7c789-aca0-45e8-f89b-5864d30c6434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 3ms/step\n",
      "Insert search tags to help users find your model (optional): conv1d\n",
      "Provide any useful notes about your model (optional): conv1d\n",
      "\n",
      "Your model has been submitted as model version 152\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 2: \n",
    "prediction_column_index=model2.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmg69UWTbMkr"
   },
   "source": [
    "### Use transfer learning with glove embeddings for at least one of these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coPHXCNjlOcn"
   },
   "source": [
    "This model actually ended up being the best performing so far, coming in at an accuracy of .81 for the test dataset. I increased the dropout percentage and it did result in further decreased overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srAFsGUtbNrz",
    "outputId": "c12707b1-39e4-41c1-eb48-4047b92bf38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-16 13:34:23--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-16 13:34:23--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-16 13:34:24--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182753 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.12MB/s    in 2m 39s  \n",
      "\n",
      "2023-04-16 13:37:03 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
    "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vh9pCJwQbVD6",
    "outputId": "98f9bb31-dbc9-4caf-8640-078ea6f349cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtxUPLWUcBcc",
    "outputId": "6a1bfb62-3ece-40ea-db64-6d48222d95e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Extract embedding data for 100 feature embedding matrix\n",
    "glove_dir = os.getcwd()\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DAOFnmDwcYz_"
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_dim = 100 # change if you use txt files using larger number of features\n",
    "\n",
    "embedding_matrix = np.zeros((10000, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < 10000:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KylcGZTgc-Zy",
    "outputId": "692f7a06-74f1-4ad5-82a7-4dbdcf492b50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 16)                7488      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,007,522\n",
      "Trainable params: 1,007,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 14s 64ms/step - loss: 0.6408 - acc: 0.6292 - val_loss: 0.8160 - val_acc: 0.3613\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 8s 47ms/step - loss: 0.4824 - acc: 0.7897 - val_loss: 0.6108 - val_acc: 0.7565\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 11s 64ms/step - loss: 0.3536 - acc: 0.8616 - val_loss: 0.6855 - val_acc: 0.7225\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 7s 43ms/step - loss: 0.2757 - acc: 0.8889 - val_loss: 0.6385 - val_acc: 0.7428\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 11s 65ms/step - loss: 0.2265 - acc: 0.9072 - val_loss: 0.6821 - val_acc: 0.7182\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 7s 43ms/step - loss: 0.1943 - acc: 0.9178 - val_loss: 0.5345 - val_acc: 0.8006\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 11s 66ms/step - loss: 0.1666 - acc: 0.9312 - val_loss: 0.6261 - val_acc: 0.7876\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 7s 42ms/step - loss: 0.1415 - acc: 0.9447 - val_loss: 0.7169 - val_acc: 0.7630\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 13s 77ms/step - loss: 0.1254 - acc: 0.9516 - val_loss: 0.9965 - val_acc: 0.6958\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 7s 42ms/step - loss: 0.1108 - acc: 0.9583 - val_loss: 0.7165 - val_acc: 0.7681\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(10000, embedding_dim, input_length=40))\n",
    "model4.add(LSTM(16, dropout=0.4, recurrent_dropout=0.4)) \n",
    "model4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "model4.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model4.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FwzBDWomiq7D"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model4, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model4.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPJo0PV0i6yP",
    "outputId": "20dec493-d156-4cbe-e4d4-0ac1a1ccbd42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 26ms/step\n",
      "Insert search tags to help users find your model (optional): lstm\n",
      "Provide any useful notes about your model (optional): lstm 100 trying again\n",
      "\n",
      "Your model has been submitted as model version 158\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 4: \n",
    "prediction_column_index=model4.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3usiCzcto1wh"
   },
   "source": [
    "### Discuss which models performed better and point out relevant hyper-parameter values for successful models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JRcn1iko4nW"
   },
   "source": [
    "In general, all of the models I tried performed very similarly. I played around with tuning the number of units in the LSTM and the dropout rate. The number of units in the LSTM honestly didn't seem to make much of a difference. The dropout rate had the clearest effect, resulting in what appeared to be a slower learning rate and decreased overfitting to the training data. My most successful models had the higher dropout rate of .4 so I view this as leading to more success. I could conceivably try to increase the dropout rate even further in subsequent models.\n",
    "\n",
    "My best performing model also used the pre-trained glove embedding layer. Again, there was only a difference of about 1% in accuracy between my best performing and second best performing models, but either way I'm sure that the additional context contained in the pre-trained embeddings is useful for preventing overfitting to training data.\n",
    "\n",
    "Another thing I experimented with was stacking layers (LSTM and conv1d) and adding batch normalization layers to some of my conv1d models. This didn't seem to make a noticeable impact and often resulted in the models getting to 100% accuracy on the training data very quickly, which I didn't necessarily view as a good thing and took as a sign of overfitting.\n",
    "\n",
    "Another thing I noticed when submitting my models to the leaderboard was that one of the top performers on the board used the adam optimizer rather than rmsprop. I tried a model using adam and saw no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2ufNVLhUSuF"
   },
   "source": [
    "## After you submit your first three models, describe your best model with your team via your team slack channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDve9fYyUblo"
   },
   "source": [
    "By Sunday, only one member of my team had also posted in the slack channel to discuss their best models. This member said that their best performing model had an embedding layer with conv1d layers. I had already tried my own models with conv1d layers, so I decided to take some inspiration from the best performing models on the leaderboard, not necessarily from my team. I saw that one of the best performers used two bidirectional LSTM layers, so I wanted to give that a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApUjrHG3rfhy"
   },
   "source": [
    "### Trying bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXvoIsZfqK6O",
    "outputId": "d156bd2a-70a0-472a-9cc0-4565587cda6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_28 (Embedding)    (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirecti  (None, 40, 32)           14976     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 16)               2624      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,017,634\n",
      "Trainable params: 1,017,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "173/173 [==============================] - 33s 138ms/step - loss: 0.6284 - acc: 0.6393 - val_loss: 0.7496 - val_acc: 0.6626\n",
      "Epoch 2/5\n",
      "173/173 [==============================] - 23s 132ms/step - loss: 0.4179 - acc: 0.8172 - val_loss: 0.7896 - val_acc: 0.6517\n",
      "Epoch 3/5\n",
      "173/173 [==============================] - 23s 134ms/step - loss: 0.2854 - acc: 0.8916 - val_loss: 0.5090 - val_acc: 0.8049\n",
      "Epoch 4/5\n",
      "173/173 [==============================] - 22s 124ms/step - loss: 0.2144 - acc: 0.9182 - val_loss: 0.5280 - val_acc: 0.7724\n",
      "Epoch 5/5\n",
      "173/173 [==============================] - 22s 126ms/step - loss: 0.1646 - acc: 0.9386 - val_loss: 0.5711 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(10000, embedding_dim, input_length=40))\n",
    "model5.add(layers.Bidirectional(layers.LSTM(16,dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model5.add(layers.Bidirectional(layers.LSTM(8)))\n",
    "model5.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model5.summary()\n",
    "\n",
    "model5.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model5.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "sqplaV4JrSEc"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model5, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model5.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9ZPTT_1rZKg",
    "outputId": "edac1d31-fbeb-427c-a44e-b0e6398a5ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 52ms/step\n",
      "Insert search tags to help users find your model (optional): bidirectional lstm\n",
      "Provide any useful notes about your model (optional): bidirectional lstm\n",
      "\n",
      "Your model has been submitted as model version 159\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 5: \n",
    "prediction_column_index=model5.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "mycompetition.submit_model(model_filepath = \"model5.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_beTTK5yVqmb"
   },
   "source": [
    "Sadly my try with bidirectional LSTM layers didn't result in a model with higher accuracy. Overall, the variance in accuracy accross all my models is only about 2%, with my worst models coming in at 79% and best at 81%. So in general I'm not seeing a huge difference being made with any of my changes in hyperparameter tuning. \n",
    "\n",
    "I'm going to try another bidirectional LSTM model but changing up the dropout amount on both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URIYFMs2WHD5",
    "outputId": "d4ea505e-0a39-4cdb-d17d-a014f545d829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 40, 32)           14976     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 16)               2624      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,017,634\n",
      "Trainable params: 1,017,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n",
      "173/173 [==============================] - 43s 197ms/step - loss: 0.6334 - acc: 0.6387 - val_loss: 0.7141 - val_acc: 0.7059\n",
      "Epoch 2/4\n",
      "173/173 [==============================] - 30s 174ms/step - loss: 0.3951 - acc: 0.8340 - val_loss: 0.7220 - val_acc: 0.7132\n",
      "Epoch 3/4\n",
      "173/173 [==============================] - 32s 185ms/step - loss: 0.2431 - acc: 0.9111 - val_loss: 0.6093 - val_acc: 0.7428\n",
      "Epoch 4/4\n",
      "173/173 [==============================] - 32s 188ms/step - loss: 0.1628 - acc: 0.9449 - val_loss: 0.7730 - val_acc: 0.7355\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten,LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(10000, embedding_dim, input_length=40))\n",
    "model6.add(Bidirectional(LSTM(16,dropout=0.6, recurrent_dropout=0.6,return_sequences=True)))\n",
    "model6.add(Bidirectional(LSTM(8,dropout=0.2, recurrent_dropout=0.2,)))\n",
    "model6.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model6.summary()\n",
    "\n",
    "model6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model6.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BNULQlrX-7R",
    "outputId": "a5fbb7f0-9a25-4936-f4ff-8021515554d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "173/173 [==============================] - 33s 189ms/step - loss: 0.0436 - acc: 0.9857 - val_loss: 1.1863 - val_acc: 0.7327\n",
      "Epoch 2/2\n",
      "173/173 [==============================] - 29s 165ms/step - loss: 0.0357 - acc: 0.9901 - val_loss: 0.9837 - val_acc: 0.7608\n"
     ]
    }
   ],
   "source": [
    "history = model6.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RQAfIX25ZMa9"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model6, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model6.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tickaUCiZRor",
    "outputId": "5c15caf4-48d7-40dc-9d9b-3a1b530159c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 22ms/step\n",
      "Insert search tags to help users find your model (optional): bidirectional lstm\n",
      "Provide any useful notes about your model (optional): bidirectional lstm with more dropout\n",
      "\n",
      "Your model has been submitted as model version 200\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 6: \n",
    "\n",
    "prediction_column_index=model6.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "mycompetition.submit_model(model_filepath = \"model6.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiLXx0asSwdW"
   },
   "source": [
    "This actually ended up being one of my worst performing models, but again the difference in accuracy was not very much. I trained this model on less epochs so maybe doing more epochs would be more effective? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySBHsg-HTPB0"
   },
   "source": [
    "## Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lewP0HITQq3"
   },
   "source": [
    "Again, all of the models I tried performed very similarly, ranging from 78% accuracy to 81% accuracy. I tried a few LSTM-based models, some models with stacked conv1d layers, added batch normalization to one, played around with adam vs. rmsprop optimization. I tried doing my own embeddings and used the glove embeddings. I also tried adding bidirectional LSTMs based on other top models I saw on the leaderboard. There was not a very clear jump in accuracy caused by tuning any of these hyperparameters. When I investigated the top performing models, I found that there was a good amount of variation in model architecture. Unfortunately, the highest performing model by 10% accuracy didn't have any model details submitted to the contest page. I wonder if it was based on transfer learning from a pre-trained sentiment classification model.\n",
    "\n",
    "Ultimately, my best performing model had one LSTM layer with .4 dropout and used the pre-trained glove embedding layer. I found the dropout hyperparameter had the most noticeable effect in changing how fast the model was learning and dealing with overfitting. If I were to continue trying to refine my models, I would probably try gridsearch with different dropout ranges.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
